# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #5 выполнил(а):
- Беспалова Полина Александровна
- РИ210910
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения.
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения.
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения.
- Выводы.

## Цель работы
Обучить ML-Agent и научиться интегрировать определенную экономическую систему в созданный проект в Unity.

## Задание 1
### Изменить параметры файла yaml-агента и определить, какие параметры и как влияют на обучение модели.
Ход работы:
- Я запустила проект, ознакомившись с его работой. Выполнила необходимые шаги, затем создала и активировала виртуальное пространство, после чего запустила обучение ML-Agent'а.
![1](https://user-images.githubusercontent.com/113704972/205062239-6961d14b-c14a-4921-a618-69651aded84d.png)
![2](https://user-images.githubusercontent.com/113704972/205062322-5bf2f881-055d-475c-9c77-a34f565c58cf.png)
![5](https://user-images.githubusercontent.com/113704972/205062366-c6c92a1b-dccc-4449-826c-1b251221128c.png)

- Далее я увеличила количество префабов TargetAreaEconomic до 12 моделей, ускорив обучение агента: 
![6!!!](https://user-images.githubusercontent.com/113704972/205062531-44e23be3-e210-4bb3-931e-c834cfa69a97.png)
![7!!!](https://user-images.githubusercontent.com/113704972/205063136-4f1f4581-e0c7-4f20-827b-f229be55b64a.png)
- Завершила обучение.

- Установила и запустила TensorBoard
![start](https://user-images.githubusercontent.com/113704972/205063234-9b799fc0-3821-41b8-802f-e2cdb448a2a0.png)

- Я начала изменять параметры файла yaml-агента, чтобы определить зависимость изменения значений, сделать выводы о различиях.

- Изначально график Cumulative Reward возрастает монотонно верх. Я уменьшила lambd  в два раза. 
![lambd](https://user-images.githubusercontent.com/113704972/205066578-1831f642-caba-4ce5-9b05-d2a199dbad97.png)
- Эта переменная используется, чтобы просчитать обобщенную оценку преимущества 'GAE'.

- Затем я изменила learning_rate:
![learningRate2](https://user-images.githubusercontent.com/113704972/205066259-09564809-8fed-4667-809e-243e06e5fb85.png)
![learningRate](https://user-images.githubusercontent.com/113704972/205066752-1f462c94-75f9-4cef-9c29-61fb13275000.png)

- Увеличина epsilon в два раза с 0.2 до 0.4:
![epsilon2](https://user-images.githubusercontent.com/113704972/205066309-dabf99c0-187f-4519-a659-ccc2f0e1c755.png)
![epsilon](https://user-images.githubusercontent.com/113704972/205066733-dca8d505-7cc8-44f9-9539-0973c6d3d2af.png)
- Он оказывает влияние на то, насколько быстро политика развивается во время обучения.


## Задание 2
### Опишите результаты, выведенные в TensorBoard:
Результаты графиков:
- 1. Episode Length - это cредняя продолжительность каждого эпизода всех агентов в среде.
- 2. Policy Loss показывает среднюю величина функции policy loss - насколько меняется политика. Значения её колеблются, но должны быть в среднем меньше единцы. Эта величина должна уменьшаться, если тренировка является успешной.
- 3. Cumulative reward - это среднее вознаграждение в эпоху для всех агентов. Он требует довольно большое количество шагов, чтобы было видео результат. График этой функции - накопительный, а значит возраграждение должно увеличиваться со временем, если пренебречь некоторыми интервалами, где график убывает и снова возрастает.
- 4. И, наконец, Value Loss - это cредняя потеря функции значения, которая показывает, насколько модель способна заранее предвидеть значения последующих состояний. График должен возрастать, пока агент обучается, затем, когда вознаграждение приходит в норму, график убывает. 

## Выводы
При выполнении лабораторной работы я настроила еще раз повторила, как работает ML-Agent, обучила его. Самое главное, я узнала, как возможно интегрировать экономическую систему в проект Unity. Попробовав поменять параметры, сделала некоторые выводы о том, как их изменение в конечном итоге повлияет на обучение ML-Agenta и скорость этого обучения. Также, разобралась в графиках и работе tensorboard, проанализировав их.

